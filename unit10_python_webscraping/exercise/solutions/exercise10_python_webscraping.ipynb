{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"frontmatter text-center\">\n",
    "<h1>Introduction to Data Science and Programming</h1>\n",
    "<h2>Exercise 10: Python Crash Course - Web scraping</h2>\n",
    "<h3>IT University of Copenhagen, Fall 2023</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: An endless Wikipedia knowledge loop\n",
    "* Task 2: Michael's links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: to import the python package `bs4` (beautifulsoup4) in the cell below, bs4 must be installed on your computer. We learned how to do this in Exercise09. See the Exercise09 notebook for detailed instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages we need\n",
    "import requests\n",
    "import bs4 # this is beautifulsoup4\n",
    "import random\n",
    "random.seed(42) # change to your personalized seed if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Wikipedia link to link\n",
    "\n",
    "In this task, we will write a function that will play a version of the [Wikipedia Game](https://www.thewikipediagame.com) for us - randomly jumping from page to page within Wikipedia. Write a function that:\n",
    "1. takes the url of a Wikipedia article, and N (number of steps), as input\n",
    "2. prints out the title of that Wikipedia article\n",
    "3. scrapes that Wikipedia article for all its `/wiki/` links (i.e. for all links to other Wikipedia articles)\n",
    "4. randomly chooses one of those `/wiki/` links, and repeats steps 1 & 2 & 3 with it\n",
    "5. this jumping from page to page repeats N times\n",
    "\n",
    "The output could look something like that: (with `random.seed(29)`, and starting at the Wikipedia page [https://en.wikipedia.org/wiki/Fibonacci_sequence](https://en.wikipedia.org/wiki/Fibonacci_sequence))\n",
    "\n",
    "<p style=\"text-align:left;\">\n",
    "    <img src=\"images/wikigame.png\" alt=\"wiki game\" width=500px>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint 1: to find the TITLE of the wikipedia article, use \n",
    "# the following method on your \"soup\":\n",
    "# .find(id=\"firstHeading\")\n",
    "# and then the .text attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint 2: to find only the wiki article content,\n",
    "# (excluding e.g. the links on the left part of the page)\n",
    "# use the following method on your soup:\n",
    "# .find(id=\"bodyContent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint 3: to check whether a link is a wikipedia link or not,\n",
    "# check whether it contains the string \"/wiki/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint 4: to create a full link, concatenate the wikipedia link like so:\n",
    "# \"https://en.wikipedia.org\" + wikilink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape one wikipedia article for all the \"internal\"\n",
    "# (wiki) links in the BODY of the article\n",
    "def scrape_wiki_article(wikiurl):\n",
    "    '''\n",
    "    takes a wikipedia url as input and scrapes it;\n",
    "    looks for other (\"internal\") wikipedia links in the BODY of the wikipedia article;\n",
    "    and returns the title of the wikipedia article (str); and a list of all wikipedia links found\n",
    "    '''\n",
    "    # make soup\n",
    "    response = requests.get(wikiurl)\n",
    "    my_text = response.content\n",
    "    soup = bs4.BeautifulSoup(my_text)\n",
    "\n",
    "    # find out the title of the article\n",
    "    article_title = soup.find(id=\"firstHeading\").text\n",
    "\n",
    "    # get only the BODY of the article\n",
    "    article_body = soup.find(id=\"bodyContent\")\n",
    "    \n",
    "    # find links in article body\n",
    "    article_links = article_body.find_all(\"a\")\n",
    "    \n",
    "    # extract the links\n",
    "    article_links = [l.get(\"href\") for l in article_links if l.get(\"href\")]\n",
    "\n",
    "    # keep only those that contain \"/wiki/\"\n",
    "    article_links = [l for l in article_links if \"/wiki/\" in l]\n",
    "\n",
    "    # exclude those that contain \":\" (these are links to wikipedia portals/help/categories...)\n",
    "    article_links = [l for l in article_links if \":\" not in l]\n",
    "\n",
    "    return article_title, article_links\n",
    "\n",
    "# function to choose a random internal wiki link, and make it \"exteral\" (by adding full link):\n",
    "def make_random_wikilink(my_list, prepend = \"https://en.wikipedia.org\"):\n",
    "    ''' \n",
    "    takes a list of internal (wiki) links as input;\n",
    "    randomly chooses one of them;\n",
    "    and prepends (by default) \"https://en.wikipedia.org\" (to convert it \n",
    "    in scrapable format understood by requests)\n",
    "    '''\n",
    "    return prepend + random.choice(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define where to start\n",
    "my_article = \"https://en.wikipedia.org/wiki/Coolio\"\n",
    "\n",
    "# define how many times to jump from page to page\n",
    "n_jumps = 20\n",
    "\n",
    "# 10 steps\n",
    "for _ in range(n_jumps):\n",
    "    \n",
    "    # scrape the current article; get title and its links\n",
    "    my_title, my_links = scrape_wiki_article(my_article)\n",
    "    \n",
    "    # print out the title\n",
    "    print(my_title)\n",
    "\n",
    "    # make the new my_article link to scrape at next iteration\n",
    "    my_article = make_random_wikilink(my_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Links to Michael from Michael's links\n",
    "\n",
    "Find out **how many of the websites linked on Michael's homepage ([http://michael.szell.net](http://michael.szell.net)) link back to his homepage**? For example, the very first link on his website is [https://en.itu.dk](https://en.itu.dk) - now you need to check whether the website [https://en.itu.dk](https://en.itu.dk) contains the link [http://michael.szell.net](http://michael.szell.net); if yes, that increases the count of linking-back websites by 1.\n",
    "\n",
    "For this task, you need to\n",
    "1. get the content of http://michael.szell.net using `requests`\n",
    "2. search the content for all links to external websites using `beautifulsoup4`\n",
    "3. for each of the links from step 2, \n",
    "    * get the content with `requests`\n",
    "    * search the content for all links with `beautifulsoup`\n",
    "    * check whether any of the links contains `michael.szell.net`\n",
    "\n",
    "> Note: When creating your list of links on Michael's website (step 2), remove `senseable.mit.edu` and `lab.moovel.com` from the list to avoid connection time out errors.\n",
    "\n",
    "Extra challenge: \"What does this have to do with Google?\" >> Read up here: [PageRank algorithm](https://en.wikipedia.org/wiki/PageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data with the \"requests\" module\n",
    "response = requests.get(\"http://michael.szell.net\")\n",
    "# the html code is in the attribute .content\n",
    "my_text = response.content\n",
    "# make your \"soup\" (use bs4 to read the html code)\n",
    "soup_michael = bs4.BeautifulSoup(my_text)\n",
    "# now we have the html code in the \"soup\" variable,\n",
    "# this \"soup\" variable can be easily searched with bs4 functions.\n",
    "print(soup_michael)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the html objects that contain links on Michael's website\n",
    "all_links = [l for l in soup_michael.find_all(\"a\")]\n",
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from all the links only the hyperlinks with .get(\"href\")\n",
    "all_hyperlinks = [l.get(\"href\") for l in all_links if l.get(\"href\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the exteral links to websites - the ones that start with \"http\", and don't contain michael.szell\n",
    "all_external_links = [link for link in all_hyperlinks if (not \"michael.szell\" in link) and (link[0:4]==\"http\")]\n",
    "all_external_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lab.moovel.com and senseable.mit.edu from the list (to avoid connection timeout)\n",
    "all_external_links = [l for l in all_external_links if \"lab.moovel.com\" not in l]\n",
    "all_external_links = [l for l in all_external_links if \"senseable.mit.edu\" not in l]\n",
    "all_external_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we need to webscrape each of the websites in `all_external_links`; find the links on each of those websites; and find out whether any of them contain the string `michael.szell.net`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that, given a website, scrapes it for its hyperlinks;\n",
    "# and returns a list of only those hyperlinks that contain a specified string.\n",
    "# we will call this function on each of the links on Michael's website.\n",
    "def find_link_on_page(my_page, my_string):\n",
    "    '''\n",
    "    takes a website (my_page; url) and a string (my_string) as input;\n",
    "    webscrapes my_page and checks whether any of its links contain my_string;\n",
    "    returns the list of links on my_page that contain my_string \n",
    "    '''\n",
    "    # get the contents of my_page and make a \"soup\"\n",
    "    my_page_response = requests.get(my_page)\n",
    "    my_page_text = my_page_response.text\n",
    "    soup=bs4.BeautifulSoup(my_page_text)\n",
    "\n",
    "    # search the contents of the page for links\n",
    "    # store all links on the page in the variable all_links\n",
    "    all_links = [l for l in soup.find_all(\"a\")]\n",
    "\n",
    "    # extract only the hyperlinks (\"href\" in html code)\n",
    "    links_href = [l.get(\"href\") for l in all_links if l.get(\"href\")]\n",
    "\n",
    "    # make a list of only those hyperlinks that contain my_string\n",
    "    links_found = [l for l in links_href if my_string in l]\n",
    "\n",
    "    return links_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a list\n",
    "websites_linking_to_michael = []\n",
    "\n",
    "# loop through all the external links\n",
    "for l in all_external_links:\n",
    "    # at each step, if the website of the link contains \"michael.szell.net\" in its links\n",
    "    links_found = find_link_on_page(l, \"michael.szell.net\")\n",
    "    # if so (if not an empty list is returned),\n",
    "    # add the current link to our list:\n",
    "    if links_found:\n",
    "        websites_linking_to_michael.append(l)\n",
    "        # and print it out\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many links link back to michael? # Count only the unique ones:\n",
    "set(websites_linking_to_michael)\n",
    "# 6! (or less, if you don't count pages/subpages as separate websites)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
